{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD: Definition and its creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of an RDD:\n",
    "In Spark, the idea is to split the data into multiple nodes i.e. machines. The partitioned data can be accessed through APIs defined in Spark. Resilient Distributed Dataset (RDD) is a representation of this distributed dataset. It is an immutable distributed collection of data. Any action performed on an RDD is transmitted to all the nodes where the action is performed on the respective chunk of data. Let's break down the term Resilient Distributed Dataset and understand the meaning and relevance of each word following the principle of LIFO.  \n",
    "\n",
    "`Dataset` refers to the variable or object created by reading a file from a disk. File could be read from either a file stored on one's local machine or data sources like HDFS, S3 or HBase.  Once the data is read in Spark, we can referece it through an RDD. These RDDs are immutable meaning that we cannot change a section of an RDD. Every time a transformation is applied to an RDD, a new RDD is created. The series of transformations thus create a data analysis pipeline.\n",
    "\n",
    "The second term is `distributed` which refers to the fact that Spark stores the data in a distributed fashion spread across a cluster of machines, say hundreds of instances on Amazon. Depending on the use case at hand (with a view to optimize perforamance), we can configure the size of each partition on the cluster. \n",
    "\n",
    "Lastly, in a distributed environment, node failures are a common phenomenon. In such an event, it is important for an application to be able to recover the work already done. `Resilient` refers to the fact that Spark keeps a track of each partition at every compute step. This way if a partition is lost, Spark can re-create it from a known set of partitions which were used to create the lost partition. This implicitly means that Spark can figure out where to start the re-computation process so that it can recover the lost partition in the least possible time.\n",
    "\n",
    "Two types of operations on RDDs are are possible which are as follows:\n",
    "1. __Transformation__ : Operations on an RDD such as filter() or map() which yield another RDD. \n",
    "2. __Action__: Operations on an RDD which trigger a computation. The result of this computation is returned to the master node or written into a stable storage system. Examples include count(), first(), take(n) or collect(). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD Creation:\n",
    "\n",
    "There are three methods to create RDDs in Spark. These are as follows:\n",
    "1. `parallelize` method: By invoking this method in the driver program we can create a parallelized collections.\n",
    "2. `textFile` method: This method creates an RDD by reading the URL of a file. It reads the file as a collection of lines. _In this notebook, we will use this method to create an RDD_.\n",
    "3. __Existing RDD__: As mentioned earlier, transformation of an existing RDD also results into an RDD. \n",
    "\n",
    "In the following paragraphs, we'll see the first two methods of creating an RDD. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `parallelize` method\n",
    "We will create an RDD by reading a list from the `driver program` i.e. `PySpark` in this case and distribute it over 3 partitions. As a next step, we'll collect the data from each of the partitions and print the list. Note that collecting the partitioned data requires the RDD to fit into the memory of the driver program. Therefore, collecting a big RDD should be done with caution. Finally, the `glom()` method allows us to see how the data is actually partitioned across the cluster of nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[[0, 1, 2], [3, 4, 5], [6, 7, 8, 9]]\n"
     ]
    }
   ],
   "source": [
    "integer_RDD = sc.parallelize(range(10), 3) # Read a list and distribute across 3 partitions\n",
    "print integer_RDD.collect() # Gather data from all partitions\n",
    "print integer_RDD.glom().collect() # View how data is distributed across partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `textFile method`\n",
    "In order to see this method, following __[this](https://github.com/jadianes/spark-py-notebooks)__ excellent tutorial on Spark for Python developers, I too will be using the reduced 10% dataset provided for the KDD'99 competition. This dataset can be downloaded from the __[UCI ML repository](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html)__.  \n",
    "\n",
    "This reduced dataset has about half a million network interactions. The file is provided as a _gzip_ file which will be downloaded locally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "f = urllib.urlretrieve (\"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\", \"kddcup.data_10_percent.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data from the gzip file. Note that the `textFile` method can read directly from a compressed file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gzip_file = \"./kddcup.data_10_percent.gz\"\n",
    "raw_data = sc.textFile(data_gzip_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the data is loaded correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "494021"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of lines in the dataset\n",
    "raw_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'0,tcp,http,SF,181,5450,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,9,9,1.00,0.00,0.11,0.00,0.00,0.00,0.00,0.00,normal.',\n",
       " u'0,tcp,http,SF,239,486,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,19,19,1.00,0.00,0.05,0.00,0.00,0.00,0.00,0.00,normal.',\n",
       " u'0,tcp,http,SF,235,1337,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,29,29,1.00,0.00,0.03,0.00,0.00,0.00,0.00,0.00,normal.',\n",
       " u'0,tcp,http,SF,219,1337,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,6,6,0.00,0.00,0.00,0.00,1.00,0.00,0.00,39,39,1.00,0.00,0.03,0.00,0.00,0.00,0.00,0.00,normal.',\n",
       " u'0,tcp,http,SF,217,2032,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,6,6,0.00,0.00,0.00,0.00,1.00,0.00,0.00,49,49,1.00,0.00,0.02,0.00,0.00,0.00,0.00,0.00,normal.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the first few lines of the dataset\n",
    "raw_data.take(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
