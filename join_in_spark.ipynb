{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join in Spark\n",
    "This notebook explains a simple join operation in Spark. Code here is a solution to a programming assignment in the Coursera course `Hadoop Platform and Application Framework`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple `join` operation\n",
    "Dataset for this section can be found in the repository. `join1_FileA.txt` and `join1_FileA.txt` are the relevant files. These are 2 different wordcount datasets. We'll be performing a join on these two files. The content of the file isn't super clear to me. So, I'm not including the description here. Anyways, since the dataset is really small, that wouldn't be a hindrance in understanding the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load datasets\n",
    "Load the two datasets and check the contents using the `collect` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileA = sc.textFile(\"./data/simple_join/join1_FileA.txt\")\n",
    "fileB = sc.textFile(\"./data/simple_join/join1_FileB.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'able,991', u'about,11', u'burger,15', u'actor,22']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileA.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Jan-01 able,5',\n",
       " u'Feb-02 about,3',\n",
       " u'Mar-03 about,8',\n",
       " u'Apr-04 able,13',\n",
       " u'Feb-22 actor,3',\n",
       " u'Feb-23 burger,5',\n",
       " u'Mar-08 burger,2',\n",
       " u'Dec-15 able,100']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileB.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapper for fileA\n",
    "Create a map function for fileA that takes a line, splits it on the comma and turns the count to an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_fileA(line):\n",
    "        # split the input line in word and count on the comma\n",
    "        key_value = line.split(\",\")\n",
    "        word = key_value[0]\n",
    "        # turn the count to an integer  \n",
    "        count = int(key_value[1])\n",
    "        return (word, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapper for fileB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_fileB(line):\n",
    "        # split the input line into word, date and count_string\n",
    "        key_value = line.split(\",\")\n",
    "        count_string = key_value[1]\n",
    "        date_word = key_value[0].split(\" \")\n",
    "        date = date_word[0]\n",
    "        word = date_word[1] \n",
    "        return (word, date + \" \" + count_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map transformations on the two files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('able', 991)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test if the split_fileA() returns a desired output\n",
    "test_line = \"able,991\"\n",
    "split_fileA(test_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fileA' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-f39d68cfefab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfileA_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfileA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_fileA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfileA_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fileA' is not defined"
     ]
    }
   ],
   "source": [
    "fileA_data = fileA.map(split_fileA)\n",
    "fileA_data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'able', u'Jan-01 5'),\n",
       " (u'about', u'Feb-02 3'),\n",
       " (u'about', u'Mar-03 8'),\n",
       " (u'able', u'Apr-04 13'),\n",
       " (u'actor', u'Feb-22 3'),\n",
       " (u'burger', u'Feb-23 5'),\n",
       " (u'burger', u'Mar-08 2'),\n",
       " (u'able', u'Dec-15 100')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileB_data = fileB.map(split_fileB)\n",
    "fileB_data.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run join\n",
    "\n",
    "The goal is to join the two datasets using the words as keys and print for each word the wordcount for a specific date and then the total output from A.\n",
    "\n",
    "Basically for each word in fileB, we would like to print the date and count from fileB but also the total count from fileA.\n",
    "\n",
    "Spark implements the join transformation that given a RDD of (K, V) pairs to be joined with another RDD of (K, W) pairs, returns a dataset that contains (K, (V, W)) pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileB_joined_fileA = fileB_data.join(fileA_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'able', (u'Jan-01 5', 991)),\n",
       " (u'able', (u'Apr-04 13', 991)),\n",
       " (u'able', (u'Dec-15 100', 991)),\n",
       " (u'burger', (u'Feb-23 5', 15)),\n",
       " (u'burger', (u'Mar-08 2', 15)),\n",
       " (u'about', (u'Feb-02 3', 11)),\n",
       " (u'about', (u'Mar-03 8', 11)),\n",
       " (u'actor', (u'Feb-22 3', 22))]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileB_joined_fileA.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced `join` operation\n",
    "Dataset can be found in the repository in `./data/advanced_join`. It consists of a bunch of files with names containing the string `gennum` and `genchan`. The gennum files contain show names and their viewers, genchan files contain show names and the channel on which they are broadcasted. We want to find out the total number of viewers across all shows for the channel BAT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read files\n",
    "We'll read both the two types of files using pattern matching, signified by the '?' in the snipppet below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Hourly_Sports,21',\n",
       " u'Hot_Talking,44',\n",
       " u'Almost_Cooking,91',\n",
       " u'Dumb_Show,186',\n",
       " u'PostModern_Sports,377']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_views_file = sc.textFile(\"./data/advanced_join/join2_gennum?.txt\")\n",
    "# Read first 5 elements. This copies some elements of an RDD back to the driver program,\n",
    "# which is the PySpark console.\n",
    "show_views_file.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Hourly_Sports,DEF',\n",
       " u'Hot_Cooking,XYZ',\n",
       " u'Almost_Talking,CAB',\n",
       " u'Dumb_Talking,MAN',\n",
       " u'PostModern_News,BAT']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_channel_file = sc.textFile(\"./data/advanced_join/join2_genchan?.txt\")\n",
    "show_channel_file.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse and split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_show_views(line):\n",
    "    \"\"\"\n",
    "    Parse and split the show files. Fields are separated by a comma.\n",
    "    \"\"\"\n",
    "    show_views = line.split(\",\")\n",
    "    show = show_views[0]\n",
    "    views = show_views[1]\n",
    "    return (show, views)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Hourly_Sports', u'21'),\n",
       " (u'Hot_Talking', u'44'),\n",
       " (u'Almost_Cooking', u'91'),\n",
       " (u'Dumb_Show', u'186'),\n",
       " (u'PostModern_Sports', u'377')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_views = show_views_file.map(split_show_views)\n",
    "show_views.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_show_channel(line):\n",
    "    \"\"\"\n",
    "    Parse and split the channel files similar to what was done for the show files. \n",
    "    \"\"\"\n",
    "    show_channel = line.split(\",\")\n",
    "    show = show_channel[0]\n",
    "    channel = show_channel[1]\n",
    "    return (show, channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Hourly_Sports', u'DEF'),\n",
       " (u'Hot_Cooking', u'XYZ'),\n",
       " (u'Almost_Talking', u'CAB'),\n",
       " (u'Dumb_Talking', u'MAN'),\n",
       " (u'PostModern_News', u'BAT')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_channel = show_channel_file.map(split_show_channel)\n",
    "show_channel.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join the datasets\n",
    "Join the two datasets with the show name as the key. Datasets can be joined in any order as long as we are consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_dataset = show_channel.join(show_views)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'PostModern_Cooking', (u'MAN', u'938')),\n",
       " (u'PostModern_Cooking', (u'MAN', u'259')),\n",
       " (u'PostModern_Cooking', (u'MAN', u'644')),\n",
       " (u'PostModern_Cooking', (u'MAN', u'161')),\n",
       " (u'PostModern_Cooking', (u'MAN', u'686'))]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_dataset.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract channel as key\n",
    "We want to find the total number of viewers by channel. So we need to create an RDD with the channel as key and all the viewer counts as a value, whichever is the show."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_channel_views(show_views_channel): \n",
    "        channel = show_views_channel[1][0]\n",
    "        views = show_views_channel[1][1]\n",
    "        return (channel, views)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function above to create an RDD of channel and viewers.\n",
    "channel_views = joined_dataset.map(extract_channel_views)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'MAN', u'938'),\n",
       " (u'MAN', u'259'),\n",
       " (u'MAN', u'644'),\n",
       " (u'MAN', u'161'),\n",
       " (u'MAN', u'686')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channel_views.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sum across all channels\n",
    "Finally, we need to sum all of the viewers for each channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_channel_views(a, b):\n",
    "        channel_views = int(a) + int(b)\n",
    "        return channel_views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'XYZ', 5208016),\n",
       " (u'DEF', 8032799),\n",
       " (u'CNO', 3941177),\n",
       " (u'BAT', 5099141),\n",
       " (u'NOX', 2583583),\n",
       " (u'CAB', 3940862),\n",
       " (u'BOB', 2591062),\n",
       " (u'ABC', 1115974),\n",
       " (u'MAN', 6566187)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copy the results back to the driver with collect. \n",
    "channel_views.reduceByKey(sum_channel_views).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
