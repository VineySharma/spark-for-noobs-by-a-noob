{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD Basic Operations- Part I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topics to be covered in this notebook are as under.\n",
    "* Transformations: `map` and `filter`\n",
    "* Action: `collect`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained in the introductory notebook, we'll use the `textFile` method to create an RDD from an existing file. I've the `gzip` file already downloaded in my machine. If you're starighway starting from this notebook, you may want to copy the two commands for downloading the aforesaid file from the introductory notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "data_gzip_file = \"./kddcup.data_10_percent.gz\"\n",
    "raw_data = sc.textFile(data_gzip_file)\n",
    "print type(raw_data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `filter` transformation\n",
    "As the name suggests, this transformation when applied to an RDD, returns an RDD with only those elements which satisfy the filtering criterion.\n",
    "\n",
    "Let's filter out some specific interactions from the dataset. The last element in each line is a flag which tells us if the connection was `normal.`, `neptune.`, `smurf.` etc. I've taken two such interactions for demo purposes.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_raw_data = raw_data.filter(lambda x: 'normal.' in x)\n",
    "smurf_raw_data = raw_data.filter(lambda x: 'smurf.' in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 97278 'normal' interactions\n",
      "There are 280790 'smurf' interactions\n",
      "Count completed in 2.274 seconds\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "t0 = time()\n",
    "normal_count = normal_raw_data.count()\n",
    "smurf_count = smurf_raw_data.count()\n",
    "tt = time() - t0\n",
    "print \"There are {} 'normal' interactions\".format(normal_count)\n",
    "print \"There are {} 'smurf' interactions\".format(smurf_count)\n",
    "print \"Count completed in {} seconds\".format(round(tt,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An astute reader would notice that we didn't measure the elapsed time for the transformation step (in this case `filter`). This is because in Spark, distributed computation actually takes place when we execute an _action_ and not during when a _transformation_ is applied to an RDD. This means that regardless of the number of transformations applied to an RDD, no computation takes place until an action is called on that RDD. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `map` transformation\n",
    "This transformation is analogous to the `lambda` function in Python. It enables us apply a function to every element in the RDD. \n",
    "\n",
    "In the code snippet below, we'll be reading the data as a comma separated value (CSV) and then pretty print the first element of the array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "Parse completed in 0.105 seconds\n",
      "[u'0',\n",
      " u'tcp',\n",
      " u'http',\n",
      " u'SF',\n",
      " u'181',\n",
      " u'5450',\n",
      " u'0',\n",
      " u'0',\n",
      " u'0',\n",
      " u'0',\n",
      " u'0',\n",
      " u'1',\n",
      " u'0',\n",
      " u'0',\n",
      " u'0',\n",
      " u'0',\n",
      " u'0',\n",
      " u'0',\n",
      " u'0',\n",
      " u'0',\n",
      " u'0',\n",
      " u'0',\n",
      " u'8',\n",
      " u'8',\n",
      " u'0.00',\n",
      " u'0.00',\n",
      " u'0.00',\n",
      " u'0.00',\n",
      " u'1.00',\n",
      " u'0.00',\n",
      " u'0.00',\n",
      " u'9',\n",
      " u'9',\n",
      " u'1.00',\n",
      " u'0.00',\n",
      " u'0.11',\n",
      " u'0.00',\n",
      " u'0.00',\n",
      " u'0.00',\n",
      " u'0.00',\n",
      " u'0.00',\n",
      " u'normal.']\n",
      "Length of 0th element of the array 42\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "csv_data = raw_data.map(lambda x: x.split(\",\"))\n",
    "print type(csv_data)\n",
    "t0 = time()\n",
    "head_rows = csv_data.take(5) # Returns an array with the first 5 elements of the csv_data\n",
    "tt = time() - t0\n",
    "print \"Parse completed in {} seconds\".format(round(tt,3))\n",
    "pprint(head_rows[0]) # Print the first element of the array\n",
    "print \"Length of 0th element of the array\", len(head_rows[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of built-in functions like `split` in the previous snippet, we can also use user-defined functions with `map`. In the snippet below, the `parse_interaction` function reads each line of the dataset and splits it by a comma. It returns a dictionary with the last element as the key and the other elements as value. Recall that each element of the array has 42 elements and the last one is the tag (e.g. normal.) which will be the key in the dictionary.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'normal.',\n",
      " [u'0',\n",
      "  u'tcp',\n",
      "  u'http',\n",
      "  u'SF',\n",
      "  u'181',\n",
      "  u'5450',\n",
      "  u'0',\n",
      "  u'0',\n",
      "  u'0',\n",
      "  u'0',\n",
      "  u'0',\n",
      "  u'1',\n",
      "  u'0',\n",
      "  u'0',\n",
      "  u'0',\n",
      "  u'0',\n",
      "  u'0',\n",
      "  u'0',\n",
      "  u'0',\n",
      "  u'0',\n",
      "  u'0',\n",
      "  u'0',\n",
      "  u'8',\n",
      "  u'8',\n",
      "  u'0.00',\n",
      "  u'0.00',\n",
      "  u'0.00',\n",
      "  u'0.00',\n",
      "  u'1.00',\n",
      "  u'0.00',\n",
      "  u'0.00',\n",
      "  u'9',\n",
      "  u'9',\n",
      "  u'1.00',\n",
      "  u'0.00',\n",
      "  u'0.11',\n",
      "  u'0.00',\n",
      "  u'0.00',\n",
      "  u'0.00',\n",
      "  u'0.00',\n",
      "  u'0.00',\n",
      "  u'normal.'])\n"
     ]
    }
   ],
   "source": [
    "def parse_interaction(line):\n",
    "    elems = line.split(\",\")\n",
    "    tag = elems[41]\n",
    "    return (tag, elems)\n",
    "\n",
    "key_csv_data = raw_data.map(parse_interaction)\n",
    "head_rows = key_csv_data.take(5)\n",
    "pprint(head_rows[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `collect` action\n",
    "`collect` returns the elements of the dataset as an array back to the driver program.\n",
    "<font color=blue>Driver program to be defined here. The idea isn't super clear to me yet. </font>. \n",
    "\n",
    "Broadly speaking, this command helps us get all the RDD elements in the memory and work with them. Needless to say, for this reason, it should be used with caution particularly when working with large RDDs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data collected in 4.205 seconds\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "all_raw_data = raw_data.collect()\n",
    "tt = time() - t0\n",
    "print \"Data collected in {} seconds\".format(round(tt,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named numpy",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d9b9b65d9f69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregression\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLabeledPoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/abhishek/spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# MLlib currently needs NumPy 1.4+, so complain if lower\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named numpy"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
