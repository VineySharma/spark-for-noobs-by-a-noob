{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordCount in Spark (work in-progress)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The WordCount problem is to read a text and then count the number of occurrence of each word. In the MapReduce paradigm, in the `map` phase, each mapper takes a line as an input and tokenizes it. It then spits out a key-value pair with the key as the word and value as 1 for each key. In the `reduce` step, each reducer sums the count of each word and emits a key value pair with each unique word as a key and the total number of occurrence of that word as a value.  \n",
    "\n",
    "In this notebook, we will read a text file with two sentences stored in the local file system. This file can be found in the `data` directory of this repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_RDD = sc.textFile('./data/textfile_1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text_RDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `map` step \n",
    "In this step, we'll be doing the following two operations:\n",
    "1. Split the sentences into tokens i.e. words. This is taken care of by the `split_word` function in the snippted below.\n",
    "2. Create key-value pairs where the key is a word and the value is a 1. This is taken care of by the `create_kv_pair` function in snippet below.\n",
    "\n",
    "`flatMap()` applies a function (in this case `split_word`) to each chunk of the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "Type returned by split_word PythonRDD[24] at RDD at PythonRDD.scala:48\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "[(u'A', 1), (u'quick', 1), (u'brown', 1), (u'fox', 1), (u'jumped', 1), (u'over', 1), (u'a', 1), (u'lazy', 1), (u'dog.', 1), (u'A', 1), (u'quick', 1), (u'brown', 1), (u'dog', 1), (u'jumped', 1), (u'over', 1), (u'a', 1), (u'lazy', 1), (u'fox.', 1)]\n",
      "<type 'list'>\n"
     ]
    }
   ],
   "source": [
    "def split_word(line):\n",
    "    return line.split()\n",
    "\n",
    "def create_kv_pair(word):\n",
    "    return (word,1)\n",
    "\n",
    "pairs_RDD = text_RDD.flatMap(split_word).map(create_kv_pair)\n",
    "print type(text_RDD.flatMap(split_word))\n",
    "print \"Type returned by split_word\", text_RDD.flatMap(split_word)\n",
    "print type(pairs_RDD)\n",
    "print pairs_RDD.collect()\n",
    "print type(pairs_RDD.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `reduce` step\n",
    "In this step, we want to count the number of occurrences of each word. So, what we need is an aggregation of counts by key. User defined function `sum_counts` and the in-built function `reduceByKey` helps us achieve this objective. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'A', 2),\n",
       " (u'dog.', 1),\n",
       " (u'lazy', 2),\n",
       " (u'over', 2),\n",
       " (u'fox', 1),\n",
       " (u'a', 2),\n",
       " (u'quick', 2),\n",
       " (u'brown', 2),\n",
       " (u'dog', 1),\n",
       " (u'jumped', 2),\n",
       " (u'fox.', 1)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sum_counts(a, b):\n",
    "    return a + b\n",
    "wordcounts_RDD = pairs_RDD.reduceByKey(sum_counts)\n",
    "wordcounts_RDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `flatMap`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'A',\n",
       " u'quick',\n",
       " u'brown',\n",
       " u'fox',\n",
       " u'jumped',\n",
       " u'over',\n",
       " u'a',\n",
       " u'lazy',\n",
       " u'dog.',\n",
       " u'A',\n",
       " u'quick',\n",
       " u'brown',\n",
       " u'dog',\n",
       " u'jumped',\n",
       " u'over',\n",
       " u'a',\n",
       " u'lazy',\n",
       " u'fox.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_RDD = text_RDD.flatMap(split_word)\n",
    "words_RDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `filter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'A', u'a', u'A', u'a']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def starts_with_a(word):\n",
    "    return word.lower().startswith(\"a\")\n",
    "words_RDD.filter(starts_with_a).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `groupByKey`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'A', <pyspark.resultiterable.ResultIterable at 0x7f4b294a3890>),\n",
       " (u'dog.', <pyspark.resultiterable.ResultIterable at 0x7f4b294a36d0>),\n",
       " (u'lazy', <pyspark.resultiterable.ResultIterable at 0x7f4b294a3bd0>),\n",
       " (u'over', <pyspark.resultiterable.ResultIterable at 0x7f4b294a3c10>),\n",
       " (u'fox', <pyspark.resultiterable.ResultIterable at 0x7f4b294a3cd0>),\n",
       " (u'a', <pyspark.resultiterable.ResultIterable at 0x7f4b294a3350>),\n",
       " (u'quick', <pyspark.resultiterable.ResultIterable at 0x7f4b294a3450>),\n",
       " (u'brown', <pyspark.resultiterable.ResultIterable at 0x7f4b291e2710>),\n",
       " (u'dog', <pyspark.resultiterable.ResultIterable at 0x7f4b294797d0>),\n",
       " (u'jumped', <pyspark.resultiterable.ResultIterable at 0x7f4b29479390>),\n",
       " (u'fox.', <pyspark.resultiterable.ResultIterable at 0x7f4b29479690>)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs_RDD.groupByKey().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: A , Values: [1, 1]\n",
      "Key: dog. , Values: [1]\n",
      "Key: lazy , Values: [1, 1]\n",
      "Key: over , Values: [1, 1]\n",
      "Key: fox , Values: [1]\n",
      "Key: a , Values: [1, 1]\n",
      "Key: quick , Values: [1, 1]\n",
      "Key: brown , Values: [1, 1]\n",
      "Key: dog , Values: [1]\n",
      "Key: jumped , Values: [1, 1]\n",
      "Key: fox. , Values: [1]\n"
     ]
    }
   ],
   "source": [
    "for k,v in pairs_RDD.groupByKey().collect():\n",
    "    print \"Key:\", k, \", Values:\", list(v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
