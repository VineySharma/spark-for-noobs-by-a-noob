{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parquet file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to `parquet` file "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parquet is an open source column oriented data store created for the Hadoop ecosystem. Wikipedia has a very good entry on column oriented database. I’ll summarise the content here so that the continuity is maintained. Please note that I've shamelessly picked the example from Wikipedia. \n",
    "\n",
    "In a database management system (DBMS), commonly and intuitively, data tables are stored as a row rather than columns. Unlike this approach, in the column oriented data storage, data tables are stored as columns. The following example will clear out the difference. A database might have a table as shown below.\n",
    "\n",
    "| RowID | EmpID   | LastName | FirstName   | Salary |\n",
    "|------|------|------|------|------|\n",
    "| 001  | 10| Smith| Joe| 40000| \n",
    "|------|------|------|------|------|\n",
    "| 002  | 12| Jones | Mary   |50000 |  \n",
    "|------|------|------|------|------|\n",
    "| 003  | 11|Johnson|Cathy|44000|\n",
    "|------|------|------|------|------|\n",
    "| 004  |22|Jones|Bob|55000|\n",
    "\n",
    "\n",
    "The 2D tabular representation of information is actually an abstraction of the manner in which data is stored in a hardware.  Storage hardware require the data to be serialized in some form. Serialization is done in such a manner that the number of seeks in the hard disk can be minimized. This is because ‘seek’ is the most expensive operation in hard disks. \n",
    "\n",
    "There are two common ways to serialize the data. One is the `row oriented` wherein all the values of first row are serialized together, then the second row and so on and so forth. Following this approach, the table above shall be serialized in the following manner:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "001:10,Smith,Joe,40000;\n",
    "002:12,Jones,Mary,50000;\n",
    "003:11,Johnson,Cathy,44000;\n",
    "004:22,Jones,Bob,55000;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike this approach, in `column oriented` approach, values of a column are serialized at a time. Using this type of serialization, data shall be stored in the following manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "10:001,12:002,11:003,22:004;\n",
    "Smith:001,Jones:002,Johnson:003,Jones:004;\n",
    "Joe:001,Mary:002,Cathy:003,Bob:004;\n",
    "40000:001,50000:002,44000:003,55000:004;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two important benefits of a column oriented data store are: One, it reduces the amount of time needed to read the data from the disk. Most of the time, we are concerned with applying a filter on a column and retrieving the data from other columns. In such scenarios, reading from a row oriented storage would mean reading all the chunks of data, regardless of whether the column is of interest. Two, the column data offers the benefit of storage size optimization. Many popular data compression schemes, makes use of the similarity of adjacent data to compress.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading a parquet file\n",
    "\n",
    "The parquet file for this notebook has been taken from __[this repo](https://github.com/jcrobak/parquet-python/tree/master/test-data)__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "LINE_LENGTH = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sparkContext=sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the file from the local file system into a SQL DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parquetFile = sqlContext.read.parquet('./data/nation.plain.parquet')\n",
    "type(parquetFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the DataFrame into an \"in-memory temporary table\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquetFile.registerTempTable(\"parquetFile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nations_all_sql = sqlContext.sql(\"SELECT * FROM parquetFile\")\n",
    "type(nations_all_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the following line, we will first convert the dataframe into an RDD and then use a `map`. Prior to Spark 2.0, spark_df.map would alias to spark_df.rdd.map(). With Spark 2.0, you must explicitly call .rdd first. This tip has been taken from __[here](https://stackoverflow.com/questions/39535447/attributeerror-dataframe-object-has-no-attribute-map)__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "nations_all = nations_all_sql.rdd.map(lambda p: \"Country: {0:15} Ipsum Comment: {1}\".format(p.name, p.comment_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Nations and Comments -- `SELECT * FROM parquetFile`\n",
      "=========================================================\n",
      "Country: ALGERIA         Ipsum Comment:  haggle. carefully final deposits detect slyly agai\n",
      "Country: ARGENTINA       Ipsum Comment: al foxes promise slyly according to the regular accounts. bold requests alon\n",
      "Country: BRAZIL          Ipsum Comment: y alongside of the pending deposits. carefully special packages are about the ironic forges. slyly special \n",
      "Country: CANADA          Ipsum Comment: eas hang ironic, silent packages. slyly regular packages are furiously over the tithes. fluffily bold\n",
      "Country: EGYPT           Ipsum Comment: y above the carefully unusual theodolites. final dugouts are quickly across the furiously regular d\n",
      "Country: ETHIOPIA        Ipsum Comment: ven packages wake quickly. regu\n",
      "Country: FRANCE          Ipsum Comment: refully final requests. regular, ironi\n",
      "Country: GERMANY         Ipsum Comment: l platelets. regular accounts x-ray: unusual, regular acco\n",
      "Country: INDIA           Ipsum Comment: ss excuses cajole slyly across the packages. deposits print aroun\n",
      "Country: INDONESIA       Ipsum Comment:  slyly express asymptotes. regular deposits haggle slyly. carefully ironic hockey players sleep blithely. carefull\n",
      "Country: IRAN            Ipsum Comment: efully alongside of the slyly final dependencies. \n",
      "Country: IRAQ            Ipsum Comment: nic deposits boost atop the quickly final requests? quickly regula\n",
      "Country: JAPAN           Ipsum Comment: ously. final, express gifts cajole a\n",
      "Country: JORDAN          Ipsum Comment: ic deposits are blithely about the carefully regular pa\n",
      "Country: KENYA           Ipsum Comment:  pending excuses haggle furiously deposits. pending, express pinto beans wake fluffily past t\n",
      "Country: MOROCCO         Ipsum Comment: rns. blithely bold courts among the closely regular packages use furiously bold platelets?\n",
      "Country: MOZAMBIQUE      Ipsum Comment: s. ironic, unusual asymptotes wake blithely r\n",
      "Country: PERU            Ipsum Comment: platelets. blithely pending dependencies use fluffily across the even pinto beans. carefully silent accoun\n",
      "Country: CHINA           Ipsum Comment: c dependencies. furiously express notornis sleep slyly regular accounts. ideas sleep. depos\n",
      "Country: ROMANIA         Ipsum Comment: ular asymptotes are about the furious multipliers. express dependencies nag above the ironically ironic account\n",
      "Country: SAUDI ARABIA    Ipsum Comment: ts. silent requests haggle. closely express packages sleep across the blithely\n",
      "Country: VIETNAM         Ipsum Comment: hely enticingly express accounts. even, final \n",
      "Country: RUSSIA          Ipsum Comment:  requests against the platelets use never according to the quickly regular pint\n",
      "Country: UNITED KINGDOM  Ipsum Comment: eans boost carefully special requests. accounts are. carefull\n",
      "Country: UNITED STATES   Ipsum Comment: y final packages. slow foxes cajole quickly. quickly silent platelets breach ironic accounts. unusual pinto be\n"
     ]
    }
   ],
   "source": [
    "print(\"All Nations and Comments -- `SELECT * FROM parquetFile`\")\n",
    "print \"=========================================================\"\n",
    "for nation in nations_all.collect():\n",
    "    print(nation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "nations_filtered_sql = sqlContext.sql(\"SELECT name FROM parquetFile WHERE name LIKE '%UNITED%'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "nations_filtered = nations_filtered_sql.rdd.map(lambda p: \"Country: {0:20}\".format(p.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================================\n",
      "Nations Filtered -- `SELECT name FROM parquetFile WHERE name LIKE '%UNITED%'`\n",
      "==============================================================================\n",
      "Country: UNITED KINGDOM      \n",
      "Country: UNITED STATES       \n"
     ]
    }
   ],
   "source": [
    "print \"==============================================================================\"\n",
    "print(\"Nations Filtered -- `SELECT name FROM parquetFile WHERE name LIKE '%UNITED%'`\")\n",
    "print \"==============================================================================\"\n",
    "for nation in nations_filtered.collect():\n",
    "    print(nation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
